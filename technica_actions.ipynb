{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "manufactured-socket",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_hands = mp.solutions.hands\n",
    "mp_pose = mp.solutions.pose\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input,LSTM,Dense,Flatten\n",
    "from tensorflow.keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "previous-pennsylvania",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "j=j=np.zeros((200,100,42))\n",
    "length=[]\n",
    " \n",
    "m=0\n",
    "for filename in os.listdir(r\"C:\\Users\\STEVE\\Desktop\\thankyou\"):\n",
    "        cap = cv2.VideoCapture(os.path.join(r\"C:\\Users\\STEVE\\Desktop\\thankyou\",filename))\n",
    "        len = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "        print(len)\n",
    "        length.append(len)\n",
    "        k=0\n",
    "        with mp_hands.Hands(\n",
    "            min_detection_confidence=0.5,\n",
    "            min_tracking_confidence=0.5) as hands:\n",
    "          while cap.isOpened():\n",
    "            success, image = cap.read()\n",
    "\n",
    "\n",
    "            # Flip the image horizontally for a later selfie-view display, and convert\n",
    "            # the BGR image to RGB.\n",
    "            image = cv2.cvtColor(cv2.flip(image, 1), cv2.COLOR_BGR2RGB)\n",
    "            # To improve performance, optionally mark the image as not writeable to\n",
    "            # pass by reference.\n",
    "            image.flags.writeable = False\n",
    "            results = hands.process(image)\n",
    "            image=cv2.resize(image,(720,480))\n",
    "\n",
    "            # Draw the hand annotations on the image.\n",
    "            image.flags.writeable = True\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "            list=[]\n",
    "\n",
    "            try:\n",
    "                 for hand_landmarks in results.multi_hand_landmarks:\n",
    "                    mp_drawing.draw_landmarks(\n",
    "                    image, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
    "\n",
    "\n",
    "\n",
    "                    for i,l in enumerate(hand_landmarks.landmark):\n",
    "                       image_height, image_width, _ = image.shape\n",
    "                       cx,cy=int(l.x*image_width),int(l.y*image_height)\n",
    "\n",
    "                       list.append((int(cx),int(cy)))\n",
    "\n",
    "            except:\n",
    "                    list.append(np.zeros((2,21)))\n",
    "              \n",
    "            pp=np.reshape(list[:21],(42,)) \n",
    "            j[m,k,:]=pp\n",
    "            k=k+1 \n",
    "            \n",
    "            if k==length[m]-1:\n",
    "                \n",
    "                break\n",
    "            if k==100:\n",
    "                break\n",
    "        m=m+1   \n",
    "           \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exciting-nightlife",
   "metadata": {},
   "outputs": [],
   "source": [
    "j[28][20]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "elegant-night",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cap=cv2.VideoCapture(r\"C:\\Users\\STEVE\\Desktop\\thankyou\\thankyou25.mp4\")\n",
    "\n",
    "length=[]\n",
    "mm=0 \n",
    "m=28\n",
    "\n",
    "for filename in os.listdir(r\"C:\\Users\\STEVE\\Desktop\\sorry\"):\n",
    "        cap = cv2.VideoCapture(os.path.join(r\"C:\\Users\\STEVE\\Desktop\\sorry\",filename))\n",
    "        len = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "        print(len)\n",
    "        length.append(len)\n",
    "        k=0\n",
    "        with mp_hands.Hands(\n",
    "            min_detection_confidence=0.5,\n",
    "            min_tracking_confidence=0.5) as hands:\n",
    "          while cap.isOpened():\n",
    "            success, image = cap.read()\n",
    "\n",
    "\n",
    "            # Flip the image horizontally for a later selfie-view display, and convert\n",
    "            # the BGR image to RGB.\n",
    "            image = cv2.cvtColor(cv2.flip(image, 1), cv2.COLOR_BGR2RGB)\n",
    "            # To improve performance, optionally mark the image as not writeable to\n",
    "            # pass by reference.\n",
    "            image.flags.writeable = False\n",
    "            results = hands.process(image)\n",
    "            image=cv2.resize(image,(720,480))\n",
    "\n",
    "            # Draw the hand annotations on the image.\n",
    "            image.flags.writeable = True\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "            list=[]\n",
    "\n",
    "            try:\n",
    "                 for hand_landmarks in results.multi_hand_landmarks:\n",
    "                    mp_drawing.draw_landmarks(\n",
    "                    image, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
    "\n",
    "\n",
    "\n",
    "                    for i,l in enumerate(hand_landmarks.landmark):\n",
    "                       image_height, image_width, _ = image.shape\n",
    "                       cx,cy=int(l.x*image_width),int(l.y*image_height)\n",
    "\n",
    "                       list.append((int(cx),int(cy)))\n",
    "\n",
    "            except:\n",
    "                    list.append(np.zeros((2,21)))\n",
    "              \n",
    "            pp=np.reshape(list[:21],(42,)) \n",
    "            j[m,k,:]=pp\n",
    "            k=k+1 \n",
    "            \n",
    "            if k==length[mm]-1:\n",
    "                \n",
    "                break\n",
    "            if k==100:\n",
    "                break\n",
    "        m=m+1\n",
    "        mm=mm+1           \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "subjective-orleans",
   "metadata": {},
   "outputs": [],
   "source": [
    "j[60][30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hollow-polyester",
   "metadata": {},
   "outputs": [],
   "source": [
    "m=60\n",
    "length=[]\n",
    "mm=0 \n",
    "\n",
    "\n",
    "for filename in os.listdir(r\"C:\\Users\\STEVE\\Desktop\\hello\"):\n",
    "        cap = cv2.VideoCapture(os.path.join(r\"C:\\Users\\STEVE\\Desktop\\hello\",filename))\n",
    "        len = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "        print(len)\n",
    "        length.append(len)\n",
    "        k=0\n",
    "        with mp_hands.Hands(\n",
    "            min_detection_confidence=0.5,\n",
    "            min_tracking_confidence=0.5) as hands:\n",
    "          while cap.isOpened():\n",
    "            success, image = cap.read()\n",
    "\n",
    "\n",
    "            # Flip the image horizontally for a later selfie-view display, and convert\n",
    "            # the BGR image to RGB.\n",
    "            image = cv2.cvtColor(cv2.flip(image, 1), cv2.COLOR_BGR2RGB)\n",
    "            # To improve performance, optionally mark the image as not writeable to\n",
    "            # pass by reference.\n",
    "            image.flags.writeable = False\n",
    "            results = hands.process(image)\n",
    "            image=cv2.resize(image,(720,480))\n",
    "\n",
    "            # Draw the hand annotations on the image.\n",
    "            image.flags.writeable = True\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "            list=[]\n",
    "\n",
    "            try:\n",
    "                 for hand_landmarks in results.multi_hand_landmarks:\n",
    "                    mp_drawing.draw_landmarks(\n",
    "                    image, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
    "\n",
    "\n",
    "\n",
    "                    for i,l in enumerate(hand_landmarks.landmark):\n",
    "                       image_height, image_width, _ = image.shape\n",
    "                       cx,cy=int(l.x*image_width),int(l.y*image_height)\n",
    "\n",
    "                       list.append((int(cx),int(cy)))\n",
    "\n",
    "            except:\n",
    "                    list.append(np.zeros((2,21)))\n",
    "              \n",
    "            pp=np.reshape(list[:21],(42,)) \n",
    "            j[m,k,:]=pp\n",
    "            k=k+1 \n",
    "            \n",
    "            if k==length[mm]-1:\n",
    "                \n",
    "                break\n",
    "            if k==100:\n",
    "                break\n",
    "        m=m+1\n",
    "        mm=mm+1\n",
    "           \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "understood-bennett",
   "metadata": {},
   "outputs": [],
   "source": [
    "j[90][50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "civic-accessory",
   "metadata": {},
   "outputs": [],
   "source": [
    "j.shape\n",
    "j4d=j.reshape(200,4200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "responsible-round",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200, 100, 42)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#np.savetxt('hacktrain_array.txt', j4d)\n",
    "j=np.loadtxt(\"hacktrain_array.txt\")\n",
    "j.shape\n",
    "j=j.reshape(200,100,42)\n",
    "j.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "accompanied-confidence",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200, 100, 42)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "j.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "statewide-harbor",
   "metadata": {},
   "outputs": [],
   "source": [
    "oo=[[1,0,0]]*28#thankyou\n",
    "ooo=[[0,1,0]]*(60-28)#sorry\n",
    "oooo=[[0,0,1]]*(90-60)#hello"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "manufactured-substance",
   "metadata": {},
   "outputs": [],
   "source": [
    "ytrain=np.concatenate([np.array(oo),np.array(ooo),np.array(oooo)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "underlying-estate",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(90, 3)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ytrain.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "after-mistress",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "x_train,y_train = shuffle(j[:90],ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "demographic-hawaiian",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(90, 100, 42)\n",
      "(90, 3)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "recovered-washington",
   "metadata": {},
   "outputs": [],
   "source": [
    "i=Input(shape=x_train[0].shape)\n",
    "x=LSTM(100)(i)\n",
    "x=LSTM(50)(i)\n",
    "x=Dense(50,activation='relu')(x)\n",
    "x=Dense(3,activation=\"softmax\")(x)\n",
    "\n",
    "model=Model(i,x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "faced-license",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=\"adam\",loss=\"binary_crossentropy\",metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "satisfactory-philip",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/80\n",
      "3/3 [==============================] - 0s 44ms/step - loss: 0.6304 - accuracy: 0.3556\n",
      "Epoch 2/80\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 0.6269 - accuracy: 0.3556\n",
      "Epoch 3/80\n",
      "3/3 [==============================] - 0s 38ms/step - loss: 0.6243 - accuracy: 0.4889\n",
      "Epoch 4/80\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.6216 - accuracy: 0.4778\n",
      "Epoch 5/80\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 0.6166 - accuracy: 0.4778\n",
      "Epoch 6/80\n",
      "3/3 [==============================] - 0s 43ms/step - loss: 0.6127 - accuracy: 0.5222\n",
      "Epoch 7/80\n",
      "3/3 [==============================] - 0s 45ms/step - loss: 0.6077 - accuracy: 0.5444\n",
      "Epoch 8/80\n",
      "3/3 [==============================] - 0s 44ms/step - loss: 0.6011 - accuracy: 0.5222\n",
      "Epoch 9/80\n",
      "3/3 [==============================] - 0s 43ms/step - loss: 0.5935 - accuracy: 0.5000\n",
      "Epoch 10/80\n",
      "3/3 [==============================] - 0s 47ms/step - loss: 0.5805 - accuracy: 0.5000\n",
      "Epoch 11/80\n",
      "3/3 [==============================] - 0s 49ms/step - loss: 0.5712 - accuracy: 0.5444\n",
      "Epoch 12/80\n",
      "3/3 [==============================] - 0s 60ms/step - loss: 0.5604 - accuracy: 0.5667\n",
      "Epoch 13/80\n",
      "3/3 [==============================] - 0s 51ms/step - loss: 0.5518 - accuracy: 0.5667\n",
      "Epoch 14/80\n",
      "3/3 [==============================] - 0s 50ms/step - loss: 0.5446 - accuracy: 0.5889\n",
      "Epoch 15/80\n",
      "3/3 [==============================] - 0s 60ms/step - loss: 0.5372 - accuracy: 0.6000\n",
      "Epoch 16/80\n",
      "3/3 [==============================] - 0s 46ms/step - loss: 0.5402 - accuracy: 0.6000\n",
      "Epoch 17/80\n",
      "3/3 [==============================] - 0s 51ms/step - loss: 0.5221 - accuracy: 0.5889\n",
      "Epoch 18/80\n",
      "3/3 [==============================] - 0s 49ms/step - loss: 0.5141 - accuracy: 0.6333\n",
      "Epoch 19/80\n",
      "3/3 [==============================] - 0s 56ms/step - loss: 0.5077 - accuracy: 0.6000\n",
      "Epoch 20/80\n",
      "3/3 [==============================] - 0s 59ms/step - loss: 0.4845 - accuracy: 0.6222\n",
      "Epoch 21/80\n",
      "3/3 [==============================] - 0s 49ms/step - loss: 0.4615 - accuracy: 0.6222\n",
      "Epoch 22/80\n",
      "3/3 [==============================] - 0s 49ms/step - loss: 0.4416 - accuracy: 0.6333\n",
      "Epoch 23/80\n",
      "3/3 [==============================] - 0s 46ms/step - loss: 0.4365 - accuracy: 0.6556\n",
      "Epoch 24/80\n",
      "3/3 [==============================] - 0s 63ms/step - loss: 0.4166 - accuracy: 0.6444\n",
      "Epoch 25/80\n",
      "3/3 [==============================] - 0s 57ms/step - loss: 0.4144 - accuracy: 0.6778\n",
      "Epoch 26/80\n",
      "3/3 [==============================] - 0s 52ms/step - loss: 0.4102 - accuracy: 0.6667\n",
      "Epoch 27/80\n",
      "3/3 [==============================] - 0s 62ms/step - loss: 0.4150 - accuracy: 0.6556\n",
      "Epoch 28/80\n",
      "3/3 [==============================] - 0s 49ms/step - loss: 0.4195 - accuracy: 0.6778\n",
      "Epoch 29/80\n",
      "3/3 [==============================] - 0s 61ms/step - loss: 0.4165 - accuracy: 0.7000\n",
      "Epoch 30/80\n",
      "3/3 [==============================] - 0s 53ms/step - loss: 0.4200 - accuracy: 0.6778\n",
      "Epoch 31/80\n",
      "3/3 [==============================] - 0s 63ms/step - loss: 0.4666 - accuracy: 0.6778\n",
      "Epoch 32/80\n",
      "3/3 [==============================] - 0s 63ms/step - loss: 0.4407 - accuracy: 0.6778\n",
      "Epoch 33/80\n",
      "3/3 [==============================] - 0s 55ms/step - loss: 0.6550 - accuracy: 0.6000\n",
      "Epoch 34/80\n",
      "3/3 [==============================] - 0s 57ms/step - loss: 0.5590 - accuracy: 0.6333\n",
      "Epoch 35/80\n",
      "3/3 [==============================] - 0s 62ms/step - loss: 0.4575 - accuracy: 0.6333\n",
      "Epoch 36/80\n",
      "3/3 [==============================] - 0s 53ms/step - loss: 0.5213 - accuracy: 0.5556\n",
      "Epoch 37/80\n",
      "3/3 [==============================] - 0s 51ms/step - loss: 0.4842 - accuracy: 0.6000\n",
      "Epoch 38/80\n",
      "3/3 [==============================] - 0s 63ms/step - loss: 0.4786 - accuracy: 0.6000\n",
      "Epoch 39/80\n",
      "3/3 [==============================] - 0s 47ms/step - loss: 0.4693 - accuracy: 0.6111\n",
      "Epoch 40/80\n",
      "3/3 [==============================] - 0s 57ms/step - loss: 0.4562 - accuracy: 0.6111\n",
      "Epoch 41/80\n",
      "3/3 [==============================] - 0s 54ms/step - loss: 0.4406 - accuracy: 0.6222\n",
      "Epoch 42/80\n",
      "3/3 [==============================] - 0s 48ms/step - loss: 0.4320 - accuracy: 0.6889\n",
      "Epoch 43/80\n",
      "3/3 [==============================] - 0s 64ms/step - loss: 0.4305 - accuracy: 0.7111\n",
      "Epoch 44/80\n",
      "3/3 [==============================] - 0s 47ms/step - loss: 0.4420 - accuracy: 0.7000\n",
      "Epoch 45/80\n",
      "3/3 [==============================] - 0s 47ms/step - loss: 0.4330 - accuracy: 0.7222\n",
      "Epoch 46/80\n",
      "3/3 [==============================] - 0s 54ms/step - loss: 0.4170 - accuracy: 0.7111\n",
      "Epoch 47/80\n",
      "3/3 [==============================] - 0s 61ms/step - loss: 0.4044 - accuracy: 0.6889\n",
      "Epoch 48/80\n",
      "3/3 [==============================] - 0s 58ms/step - loss: 0.4010 - accuracy: 0.7111\n",
      "Epoch 49/80\n",
      "3/3 [==============================] - 0s 46ms/step - loss: 0.3993 - accuracy: 0.7222\n",
      "Epoch 50/80\n",
      "3/3 [==============================] - 0s 59ms/step - loss: 0.3972 - accuracy: 0.7222\n",
      "Epoch 51/80\n",
      "3/3 [==============================] - 0s 56ms/step - loss: 0.3926 - accuracy: 0.7111\n",
      "Epoch 52/80\n",
      "3/3 [==============================] - 0s 64ms/step - loss: 0.3884 - accuracy: 0.7111\n",
      "Epoch 53/80\n",
      "3/3 [==============================] - 0s 51ms/step - loss: 0.3866 - accuracy: 0.7222\n",
      "Epoch 54/80\n",
      "3/3 [==============================] - 0s 46ms/step - loss: 0.3839 - accuracy: 0.7333\n",
      "Epoch 55/80\n",
      "3/3 [==============================] - 0s 59ms/step - loss: 0.3805 - accuracy: 0.7444\n",
      "Epoch 56/80\n",
      "3/3 [==============================] - 0s 48ms/step - loss: 0.3788 - accuracy: 0.7333\n",
      "Epoch 57/80\n",
      "3/3 [==============================] - 0s 46ms/step - loss: 0.3776 - accuracy: 0.7444\n",
      "Epoch 58/80\n",
      "3/3 [==============================] - 0s 46ms/step - loss: 0.3802 - accuracy: 0.7444\n",
      "Epoch 59/80\n",
      "3/3 [==============================] - 0s 62ms/step - loss: 0.3810 - accuracy: 0.7222\n",
      "Epoch 60/80\n",
      "3/3 [==============================] - 0s 68ms/step - loss: 0.4008 - accuracy: 0.7000\n",
      "Epoch 61/80\n",
      "3/3 [==============================] - 0s 67ms/step - loss: 0.4026 - accuracy: 0.7000\n",
      "Epoch 62/80\n",
      "3/3 [==============================] - 0s 62ms/step - loss: 0.3961 - accuracy: 0.7333\n",
      "Epoch 63/80\n",
      "3/3 [==============================] - 0s 56ms/step - loss: 0.3930 - accuracy: 0.7667\n",
      "Epoch 64/80\n",
      "3/3 [==============================] - 0s 50ms/step - loss: 0.3846 - accuracy: 0.7778\n",
      "Epoch 65/80\n",
      "3/3 [==============================] - 0s 64ms/step - loss: 0.3818 - accuracy: 0.7778\n",
      "Epoch 66/80\n",
      "3/3 [==============================] - 0s 63ms/step - loss: 0.3582 - accuracy: 0.7444\n",
      "Epoch 67/80\n",
      "3/3 [==============================] - 0s 45ms/step - loss: 0.3628 - accuracy: 0.7667\n",
      "Epoch 68/80\n",
      "3/3 [==============================] - 0s 49ms/step - loss: 0.3564 - accuracy: 0.7667\n",
      "Epoch 69/80\n",
      "3/3 [==============================] - 0s 48ms/step - loss: 0.3573 - accuracy: 0.7556\n",
      "Epoch 70/80\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.3579 - accuracy: 0.7778\n",
      "Epoch 71/80\n",
      "3/3 [==============================] - 0s 50ms/step - loss: 0.3479 - accuracy: 0.7667\n",
      "Epoch 72/80\n",
      "3/3 [==============================] - 0s 52ms/step - loss: 0.3432 - accuracy: 0.8000\n",
      "Epoch 73/80\n",
      "3/3 [==============================] - 0s 58ms/step - loss: 0.3441 - accuracy: 0.8000\n",
      "Epoch 74/80\n",
      "3/3 [==============================] - 0s 60ms/step - loss: 0.3410 - accuracy: 0.7778\n",
      "Epoch 75/80\n",
      "3/3 [==============================] - 0s 54ms/step - loss: 0.3342 - accuracy: 0.8000\n",
      "Epoch 76/80\n",
      "3/3 [==============================] - 0s 58ms/step - loss: 0.3280 - accuracy: 0.8000\n",
      "Epoch 77/80\n",
      "3/3 [==============================] - 0s 60ms/step - loss: 0.3277 - accuracy: 0.8000\n",
      "Epoch 78/80\n",
      "3/3 [==============================] - 0s 60ms/step - loss: 0.3189 - accuracy: 0.8111\n",
      "Epoch 79/80\n",
      "3/3 [==============================] - 0s 63ms/step - loss: 0.3130 - accuracy: 0.8111\n",
      "Epoch 80/80\n",
      "3/3 [==============================] - 0s 57ms/step - loss: 0.3071 - accuracy: 0.8111\n"
     ]
    }
   ],
   "source": [
    "r=model.fit(x_train,y_train,epochs=80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "vocational-information",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 42)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "j.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "talented-anxiety",
   "metadata": {},
   "outputs": [],
   "source": [
    "def action(pr):\n",
    "    qq=0\n",
    "    count=0\n",
    "    for q in range(0,3):\n",
    "\n",
    "        if pr[0][q]>qq:\n",
    "            qq=pr[0][q]\n",
    "            count=q\n",
    "     \n",
    "    if count==0:\n",
    "        cc=\"Thank You\"\n",
    "      \n",
    "       \n",
    "    elif count==1:\n",
    "        cc=\"Sorry\"\n",
    "        \n",
    "    elif count==2:\n",
    "        cc=\"Hello\"\n",
    "      \n",
    "    return cc     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "improved-bulgarian",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.63902193, 0.2788387 , 0.08213948]], dtype=float32)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test=np.expand_dims(j, axis=0)\n",
    "predictions=model.predict(test[0:1,:100])\n",
    "predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "operational-green",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "  \n",
    "\n",
    "   \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "awful-bernard",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-25-9dcd83c8a168>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m         \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'MediaPipe Hands'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 64\u001b[1;33m         \u001b[1;32mif\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwaitKey\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m&\u001b[0m \u001b[1;36m0xFF\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m27\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     65\u001b[0m               \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m \u001b[0mcap\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "cap=cv2.VideoCapture(0)\n",
    "if not cap.isOpened():\n",
    "    raise IOError(\"cannot open video\")\n",
    "   \n",
    "j=np.zeros((500,42))\n",
    "k=0\n",
    "with mp_hands.Hands(\n",
    "        min_detection_confidence=0.5,\n",
    "        min_tracking_confidence=0.5) as hands:\n",
    "      while cap.isOpened():\n",
    "        success, image = cap.read()\n",
    "\n",
    "\n",
    "        # Flip the image horizontally for a later selfie-view display, and convert\n",
    "        # the BGR image to RGB.\n",
    "        image = cv2.cvtColor(cv2.flip(image, 1), cv2.COLOR_BGR2RGB)\n",
    "        # To improve performance, optionally mark the image as not writeable to\n",
    "        # pass by reference.\n",
    "        image.flags.writeable = False\n",
    "        results = hands.process(image)\n",
    "        image=cv2.resize(image,(720,480))\n",
    "\n",
    "        # Draw the hand annotations on the image.\n",
    "        image.flags.writeable = True\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "        list=[]\n",
    "\n",
    "        try:\n",
    "             for hand_landmarks in results.multi_hand_landmarks:\n",
    "                mp_drawing.draw_landmarks(\n",
    "                image, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
    "\n",
    "\n",
    "\n",
    "                for i,l in enumerate(hand_landmarks.landmark):\n",
    "                   image_height, image_width, _ = image.shape\n",
    "                   cx,cy=int(l.x*image_width),int(l.y*image_height)\n",
    "\n",
    "                   list.append((int(cx),int(cy)))\n",
    "\n",
    "        except:\n",
    "                list.append(np.zeros((2,21)))\n",
    "           \n",
    "        pp=np.reshape(list[:21],(42,)) \n",
    "        j[k,:]=pp\n",
    "        k=k+1   \n",
    "        if k<=100:\n",
    "             cv2.putText(image,\"ANALYSING....\" , ((200,15)),cv2.FONT_HERSHEY_SIMPLEX, 0.5,(0, 0,255), 2)\n",
    "               \n",
    "        else:       \n",
    "            test=np.expand_dims(j, axis=0)\n",
    "            pred=model.predict(test[0:1,:-100])\n",
    "            countt=action(pred)\n",
    "            cv2.putText(image,countt , ((250,15)),cv2.FONT_HERSHEY_SIMPLEX, 0.5,(255, 255,0), 2)\n",
    "        \n",
    "        cv2.imshow('MediaPipe Hands', image)\n",
    "        if cv2.waitKey(5) & 0xFF == 27:\n",
    "              break\n",
    "cap.release()\n",
    "            \n",
    "    \n",
    "           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "isolated-leather",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "passing-trout",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "violent-privacy",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "scheduled-austria",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lucky-politics",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
